{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9dd8862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    user_query: str\n",
    "    plan: list[str]\n",
    "    current_step: int\n",
    "    working_memory: dict\n",
    "    invoked_agents: list[str]\n",
    "    final_answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c721c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_REGISTRY = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "13bd47eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "rag_llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b78b7e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SriHarsha\\AppData\\Local\\Temp\\ipykernel_15632\\4065827976.py:14: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # or Groq embeddings if available\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"C:/Users/SriHarsha/Desktop/POC/Q2. LG_Multi_model/home_loan.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "# Create vector store\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_agent(state: AgentState) -> AgentState:\n",
    "    query = state[\"user_query\"]\n",
    "\n",
    "    # Retrieve relevant context from vector DB\n",
    "    docs = vectorstore.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Extract ONLY factual information needed from the document.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Do NOT calculate. Just extract facts.\n",
    "    \"\"\"\n",
    "\n",
    "    extracted_info = rag_llm.invoke(prompt).content\n",
    "\n",
    "    state[\"working_memory\"][\"rag_output\"] = extracted_info\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "253694d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_REGISTRY[\"rag_agent\"] = rag_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "daec58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "math_llm = ChatOpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ[\"NVIDIA_API_KEY\"],\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "114475d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_agent(state: AgentState) -> AgentState:\n",
    "    facts = state[\"working_memory\"].get(\"rag_output\", \"\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Perform calculations based on the information below.\n",
    "    Return ONLY the final numeric result.\n",
    "\n",
    "    Information:\n",
    "    {facts}\n",
    "    \"\"\"\n",
    "\n",
    "    result = math_llm.invoke(prompt).content\n",
    "    state[\"working_memory\"][\"math_result\"] = result\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32d8a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_REGISTRY[\"math_agent\"] = math_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb6cd6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "news_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b1bc799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def news_agent(state: AgentState) -> AgentState:\n",
    "#     prompt = f\"\"\"\n",
    "#     Provide the latest news updates on the following topic:\n",
    "\n",
    "#     Topic: {state['user_query']}\n",
    "\n",
    "#     Summarize key points concisely.\n",
    "#     \"\"\"\n",
    "#     news = news_llm.invoke(prompt).content\n",
    "#     return {\n",
    "#         **state,\n",
    "#         \"response\": news,\n",
    "#         \"iteration\": state[\"iteration\"] + 1\n",
    "#     }\n",
    "\n",
    "\n",
    "def news_agent(state: AgentState) -> AgentState:\n",
    "    query = state[\"user_query\"]\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Provide the latest factual news related to:\n",
    "    \"{query}\"\n",
    "\n",
    "    Keep it concise and factual.\n",
    "    \"\"\"\n",
    "\n",
    "    news = news_llm.invoke(prompt).content\n",
    "    state[\"working_memory\"][\"news\"] = news\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "080c6dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_REGISTRY[\"news_agent\"] = news_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15eaa314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "planner_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f3f1d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_planner(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Uses an LLM to decide which agents should run and in what order.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI planner.\n",
    "\n",
    "    User query:\n",
    "    \"{state['user_query']}\"\n",
    "\n",
    "    Decide which agents are required and in what order.\n",
    "\n",
    "    Available agents:\n",
    "    - rag_agent (document extraction)\n",
    "    - math_agent (calculations)\n",
    "    - news_agent (latest news)\n",
    "\n",
    "    Output ONLY a Python list, example:\n",
    "    [\"rag_agent\", \"math_agent\"]\n",
    "    \"\"\"\n",
    "\n",
    "    plan_text = planner_llm.invoke(prompt).content.strip()\n",
    "    plan = eval(plan_text)  # safe if LLM constrained\n",
    "\n",
    "    state[\"plan\"] = plan\n",
    "    state[\"current_step\"] = 0\n",
    "    state[\"working_memory\"] = {}\n",
    "    state[\"final_answer\"] = \"\"\n",
    "    state[\"invoked_agents\"] = []\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e160abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def controller(state: AgentState) -> AgentState:\n",
    "    # ---- END CONDITION ----\n",
    "    if state[\"current_step\"] == len(state[\"plan\"]):\n",
    "        state[\"next\"] = \"end\"\n",
    "        return state\n",
    "\n",
    "    if state[\"current_step\"] > len(state[\"plan\"]):\n",
    "        state[\"next\"] = \"end\"\n",
    "        return state\n",
    "\n",
    "    # ---- Execute agent ----\n",
    "    agent_name = state[\"plan\"][state[\"current_step\"]]\n",
    "\n",
    "    if len(state[\"invoked_agents\"]) <= state[\"current_step\"]:\n",
    "        state[\"invoked_agents\"].append(agent_name)\n",
    "\n",
    "    agent_fn = AGENT_REGISTRY[agent_name]\n",
    "    state = agent_fn(state)\n",
    "\n",
    "    state[\"current_step\"] += 1\n",
    "    state[\"next\"] = \"controller\"\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75fd475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node(\"supervisor\", supervisor_planner)\n",
    "graph.add_node(\"controller\", controller)\n",
    "\n",
    "graph.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph.add_edge(\"supervisor\", \"controller\")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"controller\",\n",
    "    lambda state: state[\"next\"],\n",
    "    {\n",
    "        \"controller\": \"controller\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "multi_agent_app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c17282ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "AGENTS INVOKED:\n",
      "rag_agent → math_agent\n",
      "\n",
      "==============================\n",
      "AGENT OUTPUTS:\n",
      "\n",
      "RAG_OUTPUT:\n",
      "- The Loan‑to‑Value (LTV) ratio for a home loan up to Rs. 30 lacs is **90 %**.  \n",
      "- The expression **11 × 883** represents the multiplication of 11 by 883.\n",
      "\n",
      "MATH_RESULT:\n",
      "Based on the information, I will perform the calculation:\n",
      "\n",
      "11 × 883 = 9723\n",
      "\n",
      "The final numeric result is: **9723**\n"
     ]
    }
   ],
   "source": [
    "user_question = input(\"Ask your question: \")\n",
    "\n",
    "initial_state: AgentState = {\n",
    "    \"user_query\": user_question,\n",
    "    \"plan\": [],\n",
    "    \"current_step\": 0,\n",
    "    \"working_memory\": {},\n",
    "    \"invoked_agents\": [],\n",
    "    \"next\": \"supervisor\"\n",
    "}\n",
    "\n",
    "result = multi_agent_app.invoke(initial_state)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"AGENTS INVOKED:\")\n",
    "print(\" → \".join(result[\"invoked_agents\"]))\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\"AGENT OUTPUTS:\")\n",
    "for agent, output in result[\"working_memory\"].items():\n",
    "    print(f\"\\n{agent.upper()}:\\n{output}\")\n",
    "# what is the LTV ratiofor home loan upto 30 lakh rupees and what it 11*883?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
