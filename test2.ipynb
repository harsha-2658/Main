{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d12c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class UserQueryState(TypedDict):\n",
    "    user_query: str\n",
    "    response: str\n",
    "    task: str  # \"rag\", \"math\", \"news\"\n",
    "    iteration: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6821f3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "rag_llm = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e6719818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SriHarsha\\AppData\\Local\\Temp\\ipykernel_7348\\4065827976.py:14: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # or Groq embeddings if available\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"C:/Users/SriHarsha/Desktop/POC/Q2. LG_Multi_model/home_loan.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "# Create vector store\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6cedb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_agent(state: UserQueryState) -> UserQueryState:\n",
    "    # Retrieve relevant chunks\n",
    "    query = state[\"user_query\"]\n",
    "    relevant_docs = vectorstore.similarity_search(query, k=3)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a document-based Q&A agent. Use the following context to answer the user question:\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    \n",
    "    Answer clearly and concisely.\n",
    "    \"\"\"\n",
    "    answer = rag_llm.invoke(prompt).content\n",
    "    return {**state, \"response\": answer, \"iteration\": state[\"iteration\"] + 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "877a1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rag_agent(state: UserQueryState) -> UserQueryState:\n",
    "#     prompt = f\"\"\"\n",
    "#     You are a document-based Q&A agent. Use the uploaded document to answer the user question:\n",
    "    \n",
    "#     Question: {state['user_query']}\n",
    "    \n",
    "#     Answer clearly and concisely.\n",
    "#     \"\"\"\n",
    "#     answer = rag_llm.invoke(prompt).content\n",
    "#     return {\n",
    "#         **state,\n",
    "#         \"response\": answer,\n",
    "#         \"iteration\": state[\"iteration\"] + 1\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b8c79911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "math_llm = ChatOpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ[\"NVIDIA_API_KEY\"],\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cb54fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def math_agent(state: UserQueryState) -> UserQueryState:\n",
    "    prompt = f\"\"\"\n",
    "    Solve the following mathematical problem step by step:\n",
    "\n",
    "    Problem: {state['user_query']}\n",
    "\n",
    "    Provide the final answer clearly.\n",
    "    \"\"\"\n",
    "    result = math_llm.invoke(prompt).content\n",
    "    return {\n",
    "        **state,\n",
    "        \"response\": result,\n",
    "        \"iteration\": state[\"iteration\"] + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20e11fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "news_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e979bc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_agent(state: UserQueryState) -> UserQueryState:\n",
    "    prompt = f\"\"\"\n",
    "    Provide the latest news updates on the following topic:\n",
    "\n",
    "    Topic: {state['user_query']}\n",
    "\n",
    "    Summarize key points concisely.\n",
    "    \"\"\"\n",
    "    news = news_llm.invoke(prompt).content\n",
    "    return {\n",
    "        **state,\n",
    "        \"response\": news,\n",
    "        \"iteration\": state[\"iteration\"] + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d82d3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "classifier_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3674aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_decision_model(state: UserQueryState) -> UserQueryState:\n",
    "    \"\"\"\n",
    "    Uses LLM to decide which agent should handle the query.\n",
    "    Updates 'task' in the state and returns the full state.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a routing assistant. The user query is:\n",
    "\n",
    "    \"{state['user_query']}\"\n",
    "\n",
    "    Decide which agent should handle this query: \n",
    "    Options: \"rag_agent\", \"math_agent\", \"news_agent\".\n",
    "    Respond with ONLY the agent name.\n",
    "    \"\"\"\n",
    "    agent_choice = classifier_llm.invoke(prompt).content.strip()\n",
    "\n",
    "    state[\"task\"] = agent_choice\n",
    "    return state  # Must return a dict, not a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d2d64364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2182dfba2c0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(UserQueryState)\n",
    "graph.add_node(\"rag_agent\", rag_agent)\n",
    "graph.add_node(\"math_agent\", math_agent)\n",
    "graph.add_node(\"news_agent\", news_agent)\n",
    "graph.add_node(\"supervisor\", supervisor_decision_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f2347922",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.set_entry_point(\"supervisor\")\n",
    "graph.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: state[\"task\"],  \n",
    "    {\n",
    "        \"rag_agent\": \"rag_agent\",\n",
    "        \"math_agent\": \"math_agent\",\n",
    "        \"news_agent\": \"news_agent\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "graph.add_edge(\"rag_agent\", END)\n",
    "graph.add_edge(\"math_agent\", END)\n",
    "graph.add_edge(\"news_agent\", END)\n",
    "\n",
    "multi_agent_graph = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ccbc710d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ RESPONSE FROM AGENT\n",
      "\n",
      "The LTV ratio for a home loan above ₹75 lacs is **75 %**.  \n",
      "Multiplying by 3 gives **225 %**.\n",
      "Task executed: rag_agent\n",
      "Iterations: 1\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"\\nEnter your query: \")\n",
    "initial_state = {\n",
    "    \"user_query\": user_query,\n",
    "    \"response\": \"\",\n",
    "    \"task\": \"\",\n",
    "    \"iteration\": 0\n",
    "}\n",
    "final_state = multi_agent_graph.invoke(initial_state)\n",
    "print(\"\\n✅ RESPONSE FROM AGENT\\n\")\n",
    "print(final_state[\"response\"])\n",
    "print(f\"Task executed: {final_state['task']}\")\n",
    "print(f\"Iterations: {final_state['iteration']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255ed734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
