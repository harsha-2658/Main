{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c269d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class BlogState(TypedDict):\n",
    "    topic: str\n",
    "    blog: str\n",
    "    review_feedback: str\n",
    "    accuracy: float\n",
    "    iteration: int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd9f521c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SriHarsha\\Desktop\\POC\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb1f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both GOOGLE_API_KEY and GEMINI_API_KEY are set. Using GOOGLE_API_KEY.\n"
     ]
    }
   ],
   "source": [
    "writer_llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.7\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0acd7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer_agent(state: BlogState) -> BlogState:\n",
    "    prompt = f\"\"\"\n",
    "    Write a high-quality technical blog on the topic:\n",
    "    \"{state['topic']}\"\n",
    "\n",
    "    Constraints:\n",
    "    - Maximum 500 words\n",
    "    - Improve based on this feedback:\n",
    "    {state.get('review_feedback', 'None')}\n",
    "    \"\"\"\n",
    "    blog = writer_llm.invoke(prompt).content\n",
    "    return {**state,\"blog\": blog,\"iteration\":state[\"iteration\"]+1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363f1716",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7571f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_llm = ChatOpenAI(\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "    api_key=os.environ[\"NVIDIA_API_KEY\"],\n",
    "    model=\"meta/llama3-8b-instruct\",\n",
    "    temperature=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60b81b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviewer_agent(state: BlogState) -> BlogState:\n",
    "    prompt = f\"\"\"\n",
    "    Review the following blog for factual and technical accuracy.\n",
    "\n",
    "    Blog:\n",
    "    {state['blog']}\n",
    "\n",
    "    Tasks:\n",
    "    1. Give improvement feedback\n",
    "    2. Provide an accuracy score from 0 to 100\n",
    "\n",
    "    Respond strictly in this format:\n",
    "    Accuracy: <number>\n",
    "    Feedback: <text>\n",
    "    \"\"\"\n",
    "    response = reviewer_llm.invoke(prompt).content\n",
    "    accuracy = float(response.split(\"Accuracy:\")[1].split(\"\\n\")[0].strip())\n",
    "    feedback = response.split(\"Feedback:\")[1].strip()\n",
    "    return {**state,\"accuracy\":accuracy,\"review_feedback\":feedback}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd13b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_decision(state: BlogState) -> str:\n",
    "    if state[\"accuracy\"] >= 85:\n",
    "        return \"end\"\n",
    "    return \"writer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eea88aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "babf46a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(BlogState)\n",
    "graph.add_node(\"writer\", writer_agent)\n",
    "graph.add_node(\"reviewer\", reviewer_agent)\n",
    "graph.set_entry_point(\"writer\")\n",
    "graph.add_edge(\"writer\", \"reviewer\")\n",
    "graph.add_conditional_edges(\n",
    "    \"reviewer\",\n",
    "    supervisor_decision,\n",
    "    {\n",
    "        \"writer\": \"writer\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "blog_graph = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b58b0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"topic\": \"Retrieval-Augmented Generation using LangGraph\",\n",
    "    \"blog\": \"\",\n",
    "    \"review_feedback\": \"\",\n",
    "    \"accuracy\": 0.0,\n",
    "    \"iteration\": 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c10360f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Retrieval-Augmented Generation: Supercharging RAG with LangGraph\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) has become a cornerstone for building robust, factual, and up-to-date Large Language Model (LLM) applications. By grounding LLM responses in external knowledge, RAG mitigates hallucinations and provides domain-specific insights. However, orchestrating complex RAG workflows – involving multiple retrieval steps, conditional logic, and self-correction – can quickly become challenging. Enter LangGraph, a powerful library built on LangChain, designed to build stateful, multi-actor applications with cyclical capabilities.\n",
      "\n",
      "### Why LangGraph for RAG?\n",
      "\n",
      "LangGraph excels at defining workflows as graphs, where each \"node\" performs a specific task (e.g., retrieve, generate, re-evaluate) and \"edges\" dictate the flow. This graph-based approach offers several advantages for RAG:\n",
      "\n",
      "*   **Modularity:** Break down complex RAG into manageable, reusable components.\n",
      "*   **State Management:** Maintain context and variables across the entire workflow, crucial for iterative processes.\n",
      "*   **Cyclical Workflows:** Implement advanced RAG patterns like self-correction, multi-hop reasoning, or tool use, where the output of one step can feed back into an earlier stage for refinement.\n",
      "*   **Visibility:** Clearly visualize the RAG process, aiding debugging and optimization.\n",
      "\n",
      "### Building a RAG Workflow with LangGraph\n",
      "\n",
      "A basic RAG workflow in LangGraph might look like this:\n",
      "\n",
      "1.  **User Input Node:** Receives the initial query.\n",
      "2.  **Retrieve Node:** Takes the query, interacts with a vector database (e.g., using LangChain's retrievers), and fetches relevant documents.\n",
      "3.  **Augment Node:** Combines the original query with the retrieved documents to form a comprehensive prompt for the LLM.\n",
      "4.  **Generate Node:** Passes the augmented prompt to an LLM (e.g., OpenAI, Anthropic) to produce the final answer.\n",
      "5.  **Output Node:** Returns the generated response.\n",
      "\n",
      "LangGraph allows you to define these nodes and connect them sequentially or conditionally. For advanced RAG, you could introduce nodes for:\n",
      "\n",
      "*   **Re-ranking:** After initial retrieval, to prioritize the most relevant documents.\n",
      "*   **Conditional Routing:** If the initial answer is low confidence, trigger another retrieval or a different generation strategy.\n",
      "*   **Self-Correction:** A node that evaluates the generated answer and, if unsatisfactory, routes back to retrieval or augmentation with a refined query.\n",
      "\n",
      "### The Power of Cycles and State\n",
      "\n",
      "The true strength of LangGraph for RAG lies in its ability to handle cycles. Imagine a RAG system that, after an initial generation, checks if the answer directly addresses the user's intent. If not, it can loop back, perhaps re-phrasing the query for retrieval or trying a different set of documents. This iterative refinement process, enabled by LangGraph's state management and conditional edges, dramatically enhances the robustness and accuracy of RAG systems. You can even integrate human-in-the-loop steps within these cycles.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "LangGraph elevates RAG from a simple sequence of steps to a sophisticated, intelligent workflow. By providing tools for modularity, stateful execution, and powerful cyclical patterns, it empowers developers to build highly resilient, adaptive, and accurate Retrieval-Augmented Generation applications. Dive into LangGraph to build the next generation of intelligent RAG systems.\n",
      "95.0\n"
     ]
    }
   ],
   "source": [
    "final_state = blog_graph.invoke(initial_state)\n",
    "print(final_state[\"blog\"])\n",
    "print(final_state[\"accuracy\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
